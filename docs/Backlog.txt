1) Ver como se realiza el finetunning/extract features en un modelo de red:
	- Extract features:
	    Se corta una red pre-entrenada en cualquier punto (normalmente se excluyen las capas FC).
		Se utiliza esta red para pasar las imagenes en el proceso de feed forward. En la capa recortada, se recupera
		el vector de características y se le aplica un flaten.
		Se utiliza el vector para entrenar un modelo:
			- Si el set de datos es pequeño y cabe en memoria se pueden utilizar SVMs o Linear Regresion.
			- Si el set de datos es grande y no cabe en memoria se realiza un algorítmo de entrenamiento adaptativo,
			  es decir, que se pueda entrenar por batches. En resumen, una CNN. Por lo tanto realizar un
			  extract features consistirá en congelar los pesos de las capas convolucionales y entrenar el nuevo modelo
			  de red creado a la salida.
   - Fine Tuning:
   		Se cortan las capas FC de una red pre-entrenada.
		Se añade una nueva arquitectura de capas FC sobre las capas convolucionales de la red. Estas capas FC nuevas
		tendrán pesos aleatorios de forma que, si se entrena la red de forma completa, con la actualización de los pesos
		de las capas Convolucionales se podrían perder las características entrenadas -> No se quiere esto. De modo
		que es habitual utilizar una estrategia de warm up para las capas FC (se congela la parte CNN de la red y
		se entrenan los pesos de las nuevas capas FC con un learning rate bajo).
		Para incrementar la accuracy, se pueden descongelar las capas CNN y reentrenar el modelo con un learning rate de
		nuevo muy bajo.

2) Crear en model la congelación de las capas
	- Ver con el include_top = False qué capas contiene cada CNN y modificar las capas adicionales del baseline.
	- Al añadir las capas nuevas, configurar el número de neuronas de las NN con la regla del pulgar:
		"A good rule of thumb is to take the square root of the previous number of nodes in the layer and then find the
	 	closest power of 2."

3) Crear en model.py los optimizadores y el learning rate utilizado para cada etapa: SGD y Adam con learning rates del
   paper.
   - Ver en callbacks lo del decay para los optimizadores.

4) Aplicar regularización L2 en la función de pérdidas -> Repasar en qué consiste.

5) Crear el pipeline de entrenar el modelo con finetunning y train from scratch con pesos aleatorios.
    - 5.1: Random Initialization: Se crea la arquitectura con los pesos entrenados de forma random y se entrena
                                  todo el modelo (unfreeze de todas las capas).
    - 5.2: Feature Extraction: Se congelan todas las capas convolucionales y se entrenan exclusivamente las capas FC.
                               (0 - fine tunning estrategy). Se utilizará un optimizador Adam durante 30 epocas.
    - 5.3: Fine Tunning: Se ponen como trainable de 1 - N capas convolucionales del modelo realizando dos fases:
                        1) Warm up: Se entrena únicamente la parte FC con optimizador Adam y 30 épocas
                        2) FineTunning: Se entrenan las capas convolucionales que aparecen como unfreeze con un
                                        optimizador SGD y un learning rate de 10-4 con weight decay de 10 y
                                        early stopping con paciencia de 20. El entreno se realiza durante 90 epocas.

6) Crear un modulo que permita distintos procesados de las imagenes almacenando su configuración:
    - Aplicar la GCN normalización en las imagenes (este procesado no está aplicado).